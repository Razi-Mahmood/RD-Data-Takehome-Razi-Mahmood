{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "#%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Write a simple (supervised) deep classifier to train and test using the dataset collected in Q1.\n",
    "How will you divide your dataset into training and test sets.\n",
    "What data-augmentation techniques will to use for out-of-distribution (unseen) images?\n",
    "c. Please test accuracy on the attached, rd_test_dataset zipped face images, and save the output to a .csv file.\n",
    "https://drive.google.com/file/d/1jcdByJPkAGq9JsgsdLqeyLwI4Yl6plOf/view? usp=sharing\n",
    "d. Explain your accuracy scores, and add analysis based on what you proposal in Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "1. We divide data into training, validate and test set using the ratios of 70%,20%,10% respectively\n",
    "2. We read image vectors through ImageDataGenerator that has built methods for generating batches of tensor image data with real-time data augmentation.\n",
    "3. The trained model had lower validation accuracy than on training data. The test data performance using the 70,20,10% split showed to be worse than either validation or training but closer to validation accuracy.\n",
    "4. Applied to the test files given on google drive. Downloaded the dataset, made a filelist, form image tensors from the image files. Ran model.predict, saved the results as a dataframe in a csv\n",
    "5. To get a decent model, I would obviously need to run this on larger data.\n",
    "6. I also tried another approach to improve the model by cropping the image regions containing only the face and building the model from the cropped images. Not sure which over is better on the test dataset given but they seem to perform similarly on their validation and test splits.\n",
    "7. Due to the small amount of training data, the model seem to have low and test validation accuracy. Need to run on larger collections to see any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "Using the whole image for classification of fake/real. Sometimes the background can also help with the detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic CNN model used for the discrimination\n",
    "def create_cnnmodel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32,(3,3),input_shape = (200,200,3),activation = 'relu',padding='same',))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import splitfolders\n",
    "import shutil\n",
    "\n",
    "def split_data(indir,outdir,modeldir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    else:\n",
    "        #delete all the models, splits, etc. to create fresh splits\n",
    "        shutil.rmtree(outdir)\n",
    "        os.makedirs(outdir)\n",
    "        \n",
    "    if not os.path.exists(modeldir):\n",
    "        os.makedirs(modeldir)\n",
    "    else:\n",
    "        shutil.rmtree(modeldir)\n",
    "        os.makedirs(modeldir)\n",
    "    splitfolders.ratio(indir,output=outdir,seed=40,ratio=(0.7,0.2,0.1), group_prefix=None)\n",
    "    \n",
    "    \n",
    "def evaluate_performance(dirname,outresult,model):\n",
    "    filelist=[]\n",
    "    for root, dirs, files in os.walk(dirname, topdown=False):\n",
    "            for name in files:\n",
    "                 if (not name.startswith(\".\")):\n",
    "                        path=root+\"/\"+name\n",
    "                        filelist.append(path)\n",
    "    #print(len(filelist),filelist)\n",
    "    x = np.array([np.array((Image.open(fname)).resize((200,200))) for fname in filelist])\n",
    "    resultarray=model.predict(x)\n",
    "    dict={\"Sample\":filelist,\"FakeorReal?\":resultarray[:,0]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    print(df.head())\n",
    "    df.to_csv(outresult)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using the training and validation splits\n",
    "def fit_model(model,trainpath,validatepath,testpath):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "    train = datagen.flow_from_directory(trainpath,\n",
    "                                    class_mode='binary',\n",
    "                                    batch_size=64,\n",
    "                                    target_size=(200,200))\n",
    "\n",
    "    validate = datagen.flow_from_directory(validatepath,\n",
    "                                    class_mode='binary',\n",
    "                                    batch_size=64,\n",
    "                                    target_size=(200,200))\n",
    "\n",
    "    history = model.fit_generator(train,\n",
    "                              validation_data=(validate),\n",
    "                              epochs = 50,\n",
    "                              steps_per_epoch=len(train),\n",
    "                              validation_steps=len(validate))\n",
    "    test = datagen.flow_from_directory(testpath,\n",
    "                                    class_mode='binary',\n",
    "                                    batch_size=64,\n",
    "                                    target_size=(200,200))\n",
    "    scoreSeg=model.evaluate_generator(test, 4,verbose=1)\n",
    "    print(scoreSeg)\n",
    "    return scoreSeg\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/model.h5\n"
     ]
    }
   ],
   "source": [
    "#all paths\n",
    "indir=\"../data/sampled\"\n",
    "splitdir=\"../data/sampledsplits\"\n",
    "modeldir=\"../models\"\n",
    "trainpath=splitdir+\"/train/\"\n",
    "validatepath=splitdir+\"/val/\"\n",
    "testpath=splitdir+\"/test/\"\n",
    "modelpath=modeldir+\"/model.h5\"\n",
    "print(modelpath)\n",
    "testdir='../data/rd_test_dataset'\n",
    "outresult=\"../results/result.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:39:31.548156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/6s/75106hxs7bddl69mnq1djf640000gn/T/ipykernel_62737/187624938.py:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 4s 771ms/step - loss: 0.8417 - accuracy: 0.5714 - val_loss: 2.6348 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 3s 521ms/step - loss: 5.3707 - accuracy: 0.5000 - val_loss: 0.7544 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.5909 - accuracy: 0.5000 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.7461 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6843 - accuracy: 0.6286 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6882 - accuracy: 0.5714 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6912 - accuracy: 0.5429 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 3s 556ms/step - loss: 0.6828 - accuracy: 0.6571 - val_loss: 0.6901 - val_accuracy: 0.7500\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 3s 562ms/step - loss: 0.6816 - accuracy: 0.5857 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6911 - accuracy: 0.5143 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.7047 - accuracy: 0.5286 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7298 - accuracy: 0.5143 - val_loss: 0.6773 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 3s 634ms/step - loss: 0.7021 - accuracy: 0.5000 - val_loss: 0.6789 - val_accuracy: 0.8000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 3s 599ms/step - loss: 0.6772 - accuracy: 0.5714 - val_loss: 0.6831 - val_accuracy: 0.6000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6721 - accuracy: 0.5286 - val_loss: 0.6830 - val_accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 3s 619ms/step - loss: 0.6785 - accuracy: 0.5571 - val_loss: 0.6819 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 3s 567ms/step - loss: 0.6828 - accuracy: 0.5429 - val_loss: 0.6784 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 3s 550ms/step - loss: 0.6536 - accuracy: 0.6143 - val_loss: 0.6703 - val_accuracy: 0.7500\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 3s 551ms/step - loss: 0.6478 - accuracy: 0.6571 - val_loss: 0.6590 - val_accuracy: 0.8500\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6502 - accuracy: 0.6143 - val_loss: 0.6600 - val_accuracy: 0.5500\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6598 - accuracy: 0.6286 - val_loss: 0.6508 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6901 - accuracy: 0.5714 - val_loss: 0.6542 - val_accuracy: 0.6000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6274 - accuracy: 0.6286 - val_loss: 0.6494 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 3s 586ms/step - loss: 0.6363 - accuracy: 0.7143 - val_loss: 0.6486 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6438 - accuracy: 0.6143 - val_loss: 0.6467 - val_accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 3s 562ms/step - loss: 0.6203 - accuracy: 0.6714 - val_loss: 0.6503 - val_accuracy: 0.7000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.5859 - accuracy: 0.7714 - val_loss: 0.6443 - val_accuracy: 0.6500\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 3s 561ms/step - loss: 0.5591 - accuracy: 0.7000 - val_loss: 0.6833 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.5782 - accuracy: 0.6571 - val_loss: 0.6206 - val_accuracy: 0.8000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.5314 - accuracy: 0.7286 - val_loss: 0.6204 - val_accuracy: 0.7000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 3s 522ms/step - loss: 0.5564 - accuracy: 0.7143 - val_loss: 0.6160 - val_accuracy: 0.7500\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 3s 566ms/step - loss: 0.5721 - accuracy: 0.6429 - val_loss: 0.6311 - val_accuracy: 0.7000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.5332 - accuracy: 0.8000 - val_loss: 0.6334 - val_accuracy: 0.7000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.5137 - accuracy: 0.8000 - val_loss: 0.6202 - val_accuracy: 0.7000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 3s 531ms/step - loss: 0.5265 - accuracy: 0.7571 - val_loss: 0.6051 - val_accuracy: 0.7500\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.4199 - accuracy: 0.8000 - val_loss: 0.5793 - val_accuracy: 0.7500\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.3579 - accuracy: 0.8714 - val_loss: 0.5604 - val_accuracy: 0.8000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.4521 - accuracy: 0.7714 - val_loss: 0.6227 - val_accuracy: 0.6500\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 3s 521ms/step - loss: 0.5132 - accuracy: 0.7714 - val_loss: 0.6047 - val_accuracy: 0.6000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 3s 510ms/step - loss: 0.4001 - accuracy: 0.7714 - val_loss: 0.6311 - val_accuracy: 0.5500\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.3420 - accuracy: 0.8857 - val_loss: 0.6071 - val_accuracy: 0.6500\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.3547 - accuracy: 0.8286 - val_loss: 0.6334 - val_accuracy: 0.7000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 3s 524ms/step - loss: 0.3615 - accuracy: 0.9000 - val_loss: 0.6500 - val_accuracy: 0.6000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.3148 - accuracy: 0.9000 - val_loss: 0.6489 - val_accuracy: 0.6500\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2675 - accuracy: 0.9000 - val_loss: 0.6487 - val_accuracy: 0.6500\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.4242 - accuracy: 0.7857 - val_loss: 0.6647 - val_accuracy: 0.6500\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2238 - accuracy: 0.8714 - val_loss: 0.6868 - val_accuracy: 0.7000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2575 - accuracy: 0.8714 - val_loss: 0.6845 - val_accuracy: 0.7500\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2243 - accuracy: 0.8857 - val_loss: 0.6710 - val_accuracy: 0.6500\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.1827 - accuracy: 0.9286 - val_loss: 0.6861 - val_accuracy: 0.6500\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6s/75106hxs7bddl69mnq1djf640000gn/T/ipykernel_62737/187624938.py:24: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  scoreSeg=model.evaluate_generator(test, 4,verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7337 - accuracy: 0.7000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\n",
      "4/4 [==============================] - 1s 223ms/step - loss: 0.7337 - accuracy: 0.7000\n",
      "[0.7336750030517578, 0.699999988079071]\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "                            Sample  FakeorReal?\n",
      "0  ../data/rd_test_dataset/016.jpg          1.0\n",
      "1  ../data/rd_test_dataset/002.jpg          1.0\n",
      "2  ../data/rd_test_dataset/003.jpg          1.0\n",
      "3  ../data/rd_test_dataset/017.jpg          1.0\n",
      "4  ../data/rd_test_dataset/001.jpg          1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#build, train and evaluate model on sample data\n",
    "#This code splits the data folder into train, validation and test splits in the 0.7,0.2,0.1 ratios\n",
    "#Evaluating the performance on the given dataset on https://drive.google.com/file/d/1jcdByJPkAGq9JsgsdLqeyLwI4Yl6plOf/view? \n",
    "#this directory is copied in data/rd_test_dataset\n",
    "split_data(indir,splitdir,modeldir)\n",
    "model=create_cnnmodel()\n",
    "fit_model(model,trainpath,validatepath,testpath)\n",
    "model.save(modelpath)\n",
    "evaluate_performance(testdir,outresult,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see the model can be read back and reused\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "load_model=tf.keras.models.load_model(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "\n",
    "Trying on only face regions as detected by a face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crops the region containing face in all the images of a directory and copies the cropped images out to another.\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "\n",
    "def crop_faces(dirname,outdir,detector):\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    for root, dirs, files in os.walk(dirname, topdown=False):\n",
    "        for name in files:\n",
    "            if (not name.startswith(\".\")):\n",
    "                path=root+\"/\"+name\n",
    "                outpath=outdir+\"/\"+name\n",
    "                #print(path)\n",
    "                pixels = pyplot.imread(path)\n",
    "                img= array_to_img(pixels)\n",
    "                faces = detector.detect_faces(pixels)\n",
    "                draw_image_with_boxes(path, faces)\n",
    "                imageBox = img.getbbox()\n",
    "                #imageBox = Image.fromarray(pixels).getbbox()\n",
    "                imwidth=imageBox[3]\n",
    "                imheight=imageBox[2]\n",
    "                if (len(faces)>=1):\n",
    "                    coordinates = tuple(faces[0]['box']) \n",
    "                    x, y, width, height = coordinates\n",
    "                    x=max(x,0)\n",
    "                    y=max(y,0)\n",
    "                    xmax=min(x+width,imwidth)\n",
    "                    ymax=min(y+height,imheight)\n",
    "                    coordinates=[x,y,xmax,ymax]\n",
    "                    #print(coordinates,pixels.shape,imageBox) \n",
    "                    try:\n",
    "                        Image.fromarray(pixels).crop(coordinates).save(outpath)\n",
    "                    except (IndexError or SystemError):\n",
    "                            print('Face detection error image ' ,path,subs,files)\n",
    "                else:\n",
    "                   # print(coordinates,pixels.shape,imageBox) \n",
    "                    try:\n",
    "                        img.save(outpath)\n",
    "                    except (IndexError or SystemError):\n",
    "                            print('Face save error image ' ,path,subs,files)\n",
    "\n",
    "def draw_image_with_boxes(filename, result_list):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "   # print(type(data),data.shape)\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each box\n",
    "    for result in result_list:\n",
    "        # get coordinates\n",
    "        x, y, width, height = result['box']\n",
    "        # create the shape\n",
    "        rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "def save_cropped_faces(dirname,outdir,detector):\n",
    "    if detector is None:\n",
    "        detector = MTCNN()\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    else:\n",
    "        #delete all the models, splits, etc. to create fresh splits\n",
    "        shutil.rmtree(outdir)\n",
    "        os.makedirs(outdir)\n",
    "    crop_faces(dirname,outdir,detector)\n",
    "    return detector\n",
    "def create_cropped_dataset(topdir,outdir):\n",
    "    realdir=topdir+\"/real\"\n",
    "    fakedir=topdir+\"/fake\"\n",
    "    realoutdir=outdir+\"/real\"\n",
    "    fakeoutdir=outdir+\"/fake\"\n",
    "    detector=save_cropped_faces(realdir,realoutdir,None)\n",
    "    save_cropped_faces(fakedir,fakeoutdir,detector)\n",
    "    return detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/modelcropped.h5\n"
     ]
    }
   ],
   "source": [
    "sampledir=\"../data/sampled\"\n",
    "cropdir=\"../data/cropped\"\n",
    "cropsplitdir=\"../data/croppedsplits\"\n",
    "modeldir=\"../models\"\n",
    "trainpath=cropsplitdir+\"/train/\"\n",
    "validatepath=cropsplitdir+\"/val/\"\n",
    "testpath=cropsplitdir+\"/test/\"\n",
    "croppedmodelpath=modeldir+\"/modelcropped.h5\"\n",
    "print(croppedmodelpath)\n",
    "testdir='../data/rd_test_dataset'\n",
    "testcropdir=\"../data/croppedtest\"\n",
    "outcropresult=\"../results/resultcropped.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 273ms/step\n",
      "                        Sample   FakeorReal?\n",
      "0  ../data/croppedtest/016.jpg  1.000000e+00\n",
      "1  ../data/croppedtest/002.jpg  1.000000e+00\n",
      "2  ../data/croppedtest/003.jpg  3.011414e-22\n",
      "3  ../data/croppedtest/017.jpg  1.000000e+00\n",
      "4  ../data/croppedtest/001.jpg  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "#Generate face-specific dataset called cropped\n",
    "#Does a split of the cropped image data\n",
    "#create a new instance of the CNN Model to test with face regions only\n",
    "#train the cropped region-based CNN model\n",
    "detector=create_cropped_dataset(sampledir,cropdir)\n",
    "split_data(cropdir,cropsplitdir,modeldir)\n",
    "croppedmodel=create_cnnmodel()\n",
    "fit_model(croppedmodel,trainpath,validatepath,testpath)\n",
    "croppedmodel.save(croppedmodelpath)\n",
    "save_cropped_faces(testdir,testcropdir,detector)\n",
    "evaluate_performance(testcropdir,outcropresult,croppedmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
